#
# ElastiCluster example: SLURM cluster with GPUs on Azure
#
# This is a complete and self-contained example that should get you
# started with running SLURM clusters with GPU-accelerated compute
# nodes on Azure.  For more info on GPU-enabled VMs on Azure, see:
# https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu
#

[cloud/azure]
provider=azure
subscription_id=****REPLACE WITH YOUR SUBSCRIPTION ID****
# see: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#get-application-id-and-authentication-key
tenant_id=***REPLACE WITH TENANT ID***
client_id=***REPLACE WITH APPLICATION KEY***
secret=***REPLACE WITH APPLICATION SECRET***

[login/azure]
# note: `image_user` depends on the `image_id` (see the `[cluster/*]` section)
image_user = ubuntu
image_user_sudo = root
image_sudo = True
# keypair used to run stuff on the nodes.
user_key_name=elasticluster
user_key_private=****REPLACE WITH PRIVATE KEY****
user_key_public=****REPLACE WITH PUBLIC KEY****


# SLURM software to be configured by Ansible
#
[setup/slurm]
provider=ansible
frontend_groups=slurm_master
# the `cuda` role installs support for NVIDIA GPUs and the CUDA
# runtime and development toolkit
compute_groups=slurm_worker,cuda

# if the NVIDIA drivers are not installed correctly, you might need to
# ElastiCluster to reboot the nodes -- this is accomplished by the
# following variable setting. (ATM, needed on CentOS/RHEL but not on Ubuntu.)
compute_var_allow_reboot=yes

# If nodes have multiple GPUs, you want to enable cgroup support in
# SLURM to be sure that a job can access only the GPUs that were
# allocated to it.
global_var_slurm_taskplugin=task/cgroup
global_var_slurm_proctracktype=proctrack/cgroup

global_var_ansible_ssh_host_key_dsa_public=''


# the `cluster` section binds together `cloud`, `login`, and `setup` info
# and then adds some cluster-specific settings
[cluster/azure-slurm-gpu]
cloud=azure
login=azure
setup=slurm
location=East US

# one frontend node is normal for SLURM/GridEngine
frontend_nodes=1
# change this as needed
compute_nodes=4

# in Azure, `image_id` is a `/`-separated 4-uple, consisting of:
# publisher (e.g., `canonical`), offer (e.g., `ubuntuserver`),
# sku (e.g., `16.04.0-LTS`), version (e.g., `latest`)
image_id=canonical/ubuntuserver/16.04.0-LTS/latest

# As of August 2019, "NC" and "ND" nodes are the ones offering CUDA-enabled GPUs.
#
# For more info, see:
# - https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/
# - https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu
flavor = Standard_NC6

security_group=default

# no need for GPUs on front-end node
[cluster/azure-slurm-gpu/frontend]
flavor = Standard_B1ms
